{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:28:24.102285Z",
     "start_time": "2019-02-28T03:28:23.570708Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:28:24.529144Z",
     "start_time": "2019-02-28T03:28:24.524157Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_all_files(rootdir, key):\n",
    "    import os\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir)  # 列出文件夹下所有的目录与文件\n",
    "    for i in range(0, len(list)):\n",
    "        path = os.path.join(rootdir, list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(list_all_files(path, key))\n",
    "        if os.path.isfile(path) and key in path:\n",
    "            _files.append(path)\n",
    "    return _files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:28:24.865274Z",
     "start_time": "2019-02-28T03:28:24.862253Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global df, df_ind\n",
    "    root = '.'\n",
    "    key = '10_'\n",
    "    files = list_all_files(root, key)\n",
    "    for f in files:\n",
    "        yield extract_signal(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:28:25.049781Z",
     "start_time": "2019-02-28T03:28:25.044764Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_signal(f):\n",
    "    data = pd.read_table(f, header=None, skiprows=1)\n",
    "    rawdata = np.array(data.iloc[:, 19:20])\n",
    "    force_flag = np.array(data.iloc[:, 2])\n",
    "    tds = np.where(np.diff(force_flag) == 1)[0]\n",
    "    # print(len(tds))\n",
    "    x_data = np.array([np.diff(rawdata[i - 3:i + 13, :], axis=0).T.flatten() for i in tds])\n",
    "    x_data = x_data[np.max(x_data, axis=1) > 20]\n",
    "    x_data = x_data.T\n",
    "    x_data = np.apply_along_axis(\n",
    "        lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)), 0, x_data\n",
    "    )\n",
    "    x_data = (x_data - 0.5) * 2\n",
    "    # print(x_data.max(), x_data.min())\n",
    "    return x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:28:25.223322Z",
     "start_time": "2019-02-28T03:28:25.220297Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    sample = np.random.choice(datas.shape[0], batch_size, False)\n",
    "    return datas[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:28:25.422761Z",
     "start_time": "2019-02-28T03:28:25.419761Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_noise():\n",
    "    return np.random.uniform(-1, 1, (batch_size, generator_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\operatorname*{\\Sigma}\\limits_{t=r+1}^s (x_{t-1}-x_t)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L=\\operatorname*{\\mathbb{E}}\\limits_{\\tilde{x}\\sim\\mathbb{P}_g} [D(\\tilde{x})]-\\operatorname*{\\mathbb{E}}\\limits_{x\\sim\\mathbb{P}_r}[D(x)]+\\lambda \\operatorname*{\\mathbb{E}}_{\\hat{x}\\sim\\mathbb{P}_{\\hat{x}}}[(\\left\\|\\nabla_{\\tilde{x}}D(\\hat{x})\\right\\|_2 -1)^2]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:28:26.038107Z",
     "start_time": "2019-02-28T03:28:26.027137Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    d_optim = torch.optim.Adam(D.parameters(), d_lr, betas=(0.5, 0.9))\n",
    "    g_optim = torch.optim.Adam(G.parameters(), g_lr, betas=(0.5, 0.9))\n",
    "\n",
    "    plt.ion()\n",
    "    wd = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        D.train(), G.train()\n",
    "        for ci in range(critic_iters):\n",
    "            data_batch = make_data()\n",
    "            gen_batch = make_noise()\n",
    "            data_batch, gen_batch = Variable(torch.FloatTensor(data_batch)), \\\n",
    "                                    Variable(torch.FloatTensor(gen_batch))\n",
    "            d_loss = -torch.mean(D(data_batch)) + torch.mean(D(G(gen_batch))) + calc_gradient_penalty(data_batch,\n",
    "                                                                                                     G(gen_batch))\n",
    "            wasserstein_distance = -torch.mean(D(G(gen_batch))) + torch.mean(D(data_batch))\n",
    "#             print(wasserstein_distance.item())\n",
    "\n",
    "            # d_loss = -torch.mean(torch.log(D(data_batch)) + torch.log(1 - D(G(gen_batch))))\n",
    "            # g_loss = torch.mean(torch.log(1 - D(G(gen_batch))))\n",
    "\n",
    "            d_optim.zero_grad()\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            d_optim.step()\n",
    "\n",
    "        data_batch = make_data()\n",
    "        gen_batch = make_noise()\n",
    "        data_batch, gen_batch = Variable(torch.FloatTensor(data_batch)), \\\n",
    "                                Variable(torch.FloatTensor(gen_batch))\n",
    "        g_loss = -torch.mean(D(G(gen_batch)))\n",
    "        g_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            D.eval(), G.eval()\n",
    "            plt.clf()\n",
    "            plt.suptitle('epoch=%d, w-dist=%.6f' % (epoch, wasserstein_distance.item()))\n",
    "            wd.append(wasserstein_distance.item())\n",
    "            for i in range(16):\n",
    "                plt.subplot(4, 4, i + 1)\n",
    "                gen_diff = G(gen_batch).detach().numpy()\n",
    "                gen_raw = np.hstack((np.cumsum(gen_diff[:, :int(data_len / 2)], axis=1),\n",
    "                                     np.cumsum(gen_diff[:, int(data_len / 2):], axis=1)))\n",
    "                plt.plot(gen_raw[i])\n",
    "                plt.xlim((0, data_len))\n",
    "                plt.ylim((-2, 2))\n",
    "            plt.pause(0.01)\n",
    "    plt.ioff()\n",
    "    plt.figure()\n",
    "    plt.plot(wd)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each sample of real x and generated x, we make\n",
    "$$ \\tilde{x}=\\alpha\\cdot x\\_real+(1-\\alpha)\\cdot x\\_gen $$\n",
    "where $ \\alpha $ comes from a uniform distribution.\n",
    "\\begin{equation}\n",
    "Gradient Penalty = \\lambda\\cdot(\\left\\|\\nabla_{\\tilde{x}}D(\\tilde{X})\\right\\|_2^2-1)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:28:26.853939Z",
     "start_time": "2019-02-28T03:28:26.850933Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(x_real, x_gen):\n",
    "    alpha = torch.rand(batch_size, 1)\n",
    "    alpha = alpha.expand(x_real.size())\n",
    "    x_hat = alpha * x_real + (1 - alpha) * x_gen\n",
    "    D_x = D(x_hat)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=D_x,\n",
    "        inputs=x_hat,\n",
    "        grad_outputs=torch.ones(D_x.size()),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    # print(gradients)\n",
    "    gradient_penalty = gp_lambda * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    # print(gradient_penalty)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-28T03:28:27.495Z"
    }
   },
   "outputs": [],
   "source": [
    "datas = load_data()\n",
    "datas = np.hstack([d for d in datas]).T\n",
    "\n",
    "batch_size = 32\n",
    "generator_len = 20\n",
    "data_len = 15\n",
    "epochs = 200000\n",
    "d_lr = 0.000001\n",
    "g_lr = 0.000001\n",
    "gp_lambda = 0.1\n",
    "critic_iters = 5\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(data_len, 32),\n",
    "    # nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    # nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 4),\n",
    "    # nn.Dropout(0.5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(generator_len, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, data_len),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "x = np.tile(np.linspace(-1, 1, data_len), [batch_size, 1])\n",
    "# make_data()\n",
    "train()\n",
    "torch.save(D, 'D.model')\n",
    "torch.save(G, 'G.model')\n",
    "\n",
    "# D_ = torch.load('D.model')\n",
    "# G_ = torch.load('G.model')\n",
    "# print(D_, G_)\n",
    "# batch_size = 1000\n",
    "# gen_data = make_noise()\n",
    "# print(gen_data.shape)\n",
    "# gen_data = G_(Variable(torch.FloatTensor(gen_data))).detach().numpy()\n",
    "# plt.ion()\n",
    "# for i in range(gen_data.shape[0]):\n",
    "#     plt.cla()\n",
    "#     plt.plot(np.cumsum(gen_data[i, :int(data_len / 2)]))\n",
    "#     plt.plot(np.cumsum(gen_data[i, int(data_len / 2):]))\n",
    "#     # gen_raw = np.hstack((np.cumsum(gen_data[:, :int(data_len / 2)], axis=1),\n",
    "#     #                      np.cumsum(gen_data[:, int(data_len / 2):], axis=1)))\n",
    "#     # plt.plot(gen_raw[i])\n",
    "#     plt.pause(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
